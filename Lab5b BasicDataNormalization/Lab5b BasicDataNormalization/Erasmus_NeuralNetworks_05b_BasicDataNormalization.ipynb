{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erasmus Neural Networks\n",
    "http://michalbereta.pl/nn\n",
    "## Basic data normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "Exacute the examples.\n",
    "\n",
    "Then, do the tasks and send back the notebook.\n",
    "\n",
    "Change the name of this notebook according to the schema: {YourSurname}\\_{YourFirstName}\\_{OriginalFileName}.\n",
    "\n",
    "Be sure to fill all places with \"YOUR ANSWER HERE\".\n",
    "\n",
    "When ready, send the notebook, with all the necessary files zipped, to the teacher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The importance of normalizing the inputs to neurons\n",
    "\n",
    "It is important to keep the input values to neurons 'not too big', such as from range [0,1] or [-1,1] or similar.\n",
    "\n",
    "The following data has huge absolute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data = np.loadtxt('ex_data.csv')\n",
    "\n",
    "labels = data[:,-1].astype(np.int)\n",
    "x = data[:,:-1].astype(np.float)\n",
    "\n",
    "print('min x=',x.min(axis=0))\n",
    "print('max x=',x.max(axis=0))\n",
    "\n",
    "#print(labels)\n",
    "#print(x)\n",
    "\n",
    "plt.plot(x[labels==1,0],x[labels==1,1],'o')\n",
    "plt.plot(x[labels==-1,0],x[labels==-1,1],'o')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with updating the weighs\n",
    "\n",
    "With strong input values, the weights can escape to +/- infinity easily.\n",
    "\n",
    "Note the huge values of updates and the huge resulting weights and bias (just after processing one example!).\n",
    "\n",
    "Note: keeping the weights and `eta` smaller and smaller, would solve the problem in some cases. However, it is better to normalize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example random weigths and bias\n",
    "w = -1 + 2*np.random.rand(2)\n",
    "b = np.random.rand()\n",
    "print('w and b:',w,b)\n",
    "\n",
    "#calculate the output for the first training example\n",
    "y = np.dot(x[0,:],w) + b\n",
    "print('y=',y)\n",
    "\n",
    "#check, what would be the update of the weights and bias according to Widrow-Hoff model\n",
    "eta = 0.1\n",
    "w_update = eta*(labels[0] - y)*x[0]\n",
    "b_update = eta*(labels[0] - y)*1.0\n",
    "print('w_update=',w_update)\n",
    "print('b_update=',b_update)\n",
    "w += w_update\n",
    "b += b_update\n",
    "print('After update:')\n",
    "print('w and b:',w,b)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing by dividing by the maximum absolute value (for each attribute/feature separately)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_divide_max(data):\n",
    "    maxs = np.amax(np.abs(data), axis=0)\n",
    "    #print(maxs)\n",
    "    return data / maxs\n",
    "\n",
    "print('min x=',x.min(axis=0))\n",
    "print('max x=',x.max(axis=0))\n",
    "\n",
    "x2 = normalize_divide_max(x)\n",
    "\n",
    "print('min x2=',x2.min(axis=0))\n",
    "print('max x2=',x2.max(axis=0))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x2[labels==1,0],x2[labels==1,1],'o')\n",
    "plt.plot(x2[labels==-1,0],x2[labels==-1,1],'o')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing by making the data in each column have 0 mean and 1 standard deviation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_mean_std(data):\n",
    "    means = np.mean(data, axis=0)\n",
    "    stds = np.std(data, axis=0)\n",
    "    #print('means', means)\n",
    "    #print('stds', stds)\n",
    "    return (data-means)/stds\n",
    "\n",
    "print('x means=',np.mean(x,axis=0))\n",
    "print('x stds=',np.std(x,axis=0))\n",
    "\n",
    "x3 = normalize_mean_std(x)\n",
    "\n",
    "print('x3 means=',np.mean(x3,axis=0))\n",
    "print('x3 stds=',np.std(x3,axis=0))\n",
    "print('min x3=',x3.min(axis=0))\n",
    "print('max x3=',x3.max(axis=0))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x3[labels==1,0],x3[labels==1,1],'o')\n",
    "plt.plot(x3[labels==-1,0],x3[labels==-1,1],'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data - important!!!\n",
    "\n",
    "\n",
    "After normalizing the training data and training your model, you have to apply the same normalization to the test examples before sending them to the model.\n",
    "\n",
    "#### Do not calculate maxs / (means, stds) for test data. Use the remembered values calculated from train data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
