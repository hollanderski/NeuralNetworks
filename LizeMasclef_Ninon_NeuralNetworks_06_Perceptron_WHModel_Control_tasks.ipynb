{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erasmus Neural Networks\n",
    "http://michalbereta.pl/nn\n",
    "## Control tasks for Perceptron and Widrow-Hoff model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "Exacute the examples.\n",
    "\n",
    "Then, do the tasks and send back the notebook.\n",
    "\n",
    "Change the name of this notebook according to the schema: {YourSurname}\\_{YourFirstName}\\_{OriginalFileName}.\n",
    "\n",
    "Be sure to fill all places with \"YOUR ANSWER HERE\".\n",
    "\n",
    "When ready, send the notebook, with all the necessary files zipped, to the teacher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to do\n",
    "\n",
    "- Fill the methods of the following classes with your implementation.\n",
    "\n",
    "- Read the comments to properly implement methods.\n",
    "\n",
    "- Avoid loops when possible. Use numpy operations on matrices and vectors, instead.\n",
    "\n",
    "- Execute the test code.\n",
    "\n",
    "- Compare the results with the expected results given.\n",
    "\n",
    "- Do not change the testing code, just the implementation od classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Online version of Perceptron learning\n",
    "\n",
    "Expected test output:\n",
    "\n",
    "`\n",
    "Loading train data...\n",
    "Train data:\n",
    "Number of examples= 500\n",
    "Number of inputs= 10\n",
    "Initial number of errors= 225\n",
    "Training...\n",
    "End of training\n",
    "Errors for train data after training= 0\n",
    "Loading test data...\n",
    "Test data:\n",
    "Number of examples= 439\n",
    "Number of inputs= 10\n",
    "Calculating answers for test data...\n",
    "Saving classifications for test data...\n",
    "Checking test error...\n",
    "Test errors= 2  ->  0.004555808656036446 %\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data...\n",
      "Train data:\n",
      "Number of examples= 500\n",
      "Number of inputs= 10\n",
      "Initial number of errors= 248\n",
      "Training...\n",
      "End of training\n",
      "Errors for train data after training= 0\n",
      "Loading test data...\n",
      "Test data:\n",
      "Number of examples= 439\n",
      "Number of inputs= 10\n",
      "Calculating answers for test data...\n",
      "Saving classifications for test data...\n",
      "Checking test error...\n",
      "Test errors= 2  ->  0.004555808656036446 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "class PerceptronOnline:\n",
    "    \"\"\"\n",
    "    This is a perceptron which can process one example at a time.\n",
    "    Assumption: class label is given as {-1, 1}\n",
    "    \"\"\"\n",
    "    def __init__(self, num_of_inputs):\n",
    "        \"\"\"Perceptron constructor\"\"\"\n",
    "        self.w =  -1 +2*np.random.rand(num_of_inputs)  #neurons' weights as columns\n",
    "        self.b = 0.5 \n",
    "        self.outs = None\n",
    "        pass\n",
    "    def InitWeights(self):\n",
    "        \"\"\"Initializes weights to random values\"\"\"\n",
    "        pass\n",
    "    def Forward(self, x): \n",
    "        \"\"\"Forward pass - calculate the output as {-1, 1} of the neuron for one example x\"\"\"\n",
    "        self.outs = np.sign(np.dot(x, self.w) + self.b)\n",
    "        return self.outs\n",
    "        pass\n",
    "    def Update(self, x, d, eta):\n",
    "        \"\"\"Calculate the output for x (one example), compare with d and update the weights if necessary\"\"\"\n",
    "        ans=self.Forward(x)\n",
    "        if(d!=ans):\n",
    "            #self.learningrate*input[j]*labels[i]\n",
    "            self.w+=eta*x*d\n",
    "            self.b+=eta*d\n",
    "            #update = eta*(d-ans)\n",
    "            #self.w += update*x\n",
    "            #self.b += update  \n",
    "        \n",
    "        pass\n",
    "    def Train(self, X, D, eta, epochs):\n",
    "        \"\"\"\n",
    "        Train for the maximum number of epochs or until the classification error is 0\n",
    "        X: matrix with examples, each examples as a row\n",
    "        D: vector of correct class labels for examples in rows of X\n",
    "        The update to the weights vector is done after processing each example\n",
    "        \"\"\"\n",
    "        n=0\n",
    "        while n<epochs and self.CalculateErrors(X,D)!=0:\n",
    "            for index, x in enumerate(X) :\n",
    "                self.Update(x,D[index],eta)\n",
    "            n+=1\n",
    "\n",
    "        pass\n",
    "    def CalculateErrors(self, X, D):\n",
    "        errors=0\n",
    "        for index, d in enumerate(D):\n",
    "            errors+=d!=self.Forward(X[index]) \n",
    "        \n",
    "        #errors=np.sum(D!=np.array(self.Forward(x) for x in X))          \n",
    "        return errors\n",
    "    \n",
    "        \"\"\"Calculates the number of errors - missclassifications\"\"\"\n",
    "        pass\n",
    "    \n",
    "##############################################################################\n",
    "#DO NOT CHANGE THE FOLLOWING CODE\n",
    "##############################################################################\n",
    "print('Loading train data...')\n",
    "train_data = np.loadtxt('train10D.csv')\n",
    "X = train_data[:,:-1]\n",
    "D = train_data[:,-1]\n",
    "num_of_inputs = X.shape[1]\n",
    "print('Train data:')\n",
    "print('Number of examples=',X.shape[0])\n",
    "print('Number of inputs=',num_of_inputs)\n",
    "\n",
    "perc = PerceptronOnline(num_of_inputs)\n",
    "perc.InitWeights()\n",
    "\n",
    "start_errors = perc.CalculateErrors(X,D)\n",
    "print('Initial number of errors=',start_errors)\n",
    "\n",
    "print('Training...')\n",
    "max_epochs = 100\n",
    "eta = 0.01\n",
    "perc.Train(X, D, eta, max_epochs)\n",
    "print('End of training')\n",
    "\n",
    "train_errors = perc.CalculateErrors(X,D)\n",
    "print('Errors for train data after training=',train_errors)\n",
    "\n",
    "print('Loading test data...')\n",
    "test_data = np.loadtxt('test10D.csv')\n",
    "print('Test data:')\n",
    "print('Number of examples=',test_data.shape[0])\n",
    "print('Number of inputs=',test_data.shape[1])\n",
    "\n",
    "print('Calculating answers for test data...')\n",
    "test_ans = []\n",
    "for x in test_data:\n",
    "    test_ans.append ( perc.Forward(x) )\n",
    "test_ans = np.array(test_ans)\n",
    "print('Saving classifications for test data...')\n",
    "np.savetxt('test_data_classifications_perconline.csv', test_ans)\n",
    "\n",
    "\n",
    "print('Checking test error...')\n",
    "true_test_labels = np.loadtxt('test10D_correct_ans.csv')\n",
    "test_errors = (true_test_labels != test_ans).sum()\n",
    "print('Test errors=',test_errors,' -> ',test_errors/float(test_data.shape[0]),'%') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 2 - Batch version of Perceptron learning\n",
    "\n",
    "Expected test output:\n",
    "\n",
    "`\n",
    "Loading train data...\n",
    "Train data:\n",
    "Number of examples= 500\n",
    "Number of inputs= 10\n",
    "Initial number of errors= 271\n",
    "Training...\n",
    "End of training\n",
    "Errors for train data after training= 0\n",
    "Loading test data...\n",
    "Test data:\n",
    "Number of examples= 439\n",
    "Number of inputs= 10\n",
    "Calculating answers for test data...\n",
    "Saving classifications for test data...\n",
    "Checking test error...\n",
    "Test errors= 0  ->  0.0 %\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data...\n",
      "Train data:\n",
      "Number of examples= 500\n",
      "Number of inputs= 10\n",
      "Initial number of errors= 177\n",
      "Training...\n",
      "End of training\n",
      "Errors for train data after training= 6\n",
      "Loading test data...\n",
      "Test data:\n",
      "Number of examples= 439\n",
      "Number of inputs= 10\n",
      "Calculating answers for test data...\n",
      "Saving classifications for test data...\n",
      "Checking test error...\n",
      "Test errors= 6  ->  0.01366742596810934 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "class PerceptronBatch:\n",
    "    \"\"\"\n",
    "    This is a perceptron which can process all examples at a time.\n",
    "    Assumption: class label is given as {-1, 1}\n",
    "    \"\"\"\n",
    "    def __init__(self, num_of_inputs):\n",
    "        \"\"\"Perceptron constructor\"\"\"\n",
    "        self.w =  -1 +2*np.random.rand(num_of_inputs)  #neurons' weights as columns\n",
    "        self.b = 0.5 \n",
    "        self.outs = None\n",
    "        pass\n",
    "    def InitWeights(self):\n",
    "        \"\"\"Initializes weights to random values\"\"\"\n",
    "        pass\n",
    "    def Forward(self, X): \n",
    "        \"\"\"\n",
    "        Forward pass - calculate the output as a vector of {-1, 1} of the neuron for all examples in X\n",
    "        X: matrix with examples as rows\n",
    "        \"\"\"\n",
    "        self.outs = np.sign(np.dot(X, self.w) + self.b)\n",
    "        #self.outs=np.dot(X, self.w) + self.b\n",
    "        #self.outs=np.where(np.dot(X, self.w) + self.b >= 0.0, 1, -1)\n",
    "        return self.outs\n",
    "        pass\n",
    "    def Update(self, X, D, eta):\n",
    "        \"\"\"Calculate the output for all examples in X (as rows), compare with D and update the weights if necessary\"\"\"\n",
    "        ans=self.Forward(X)\n",
    "     \n",
    "        if(np.sum(ans!=D)):\n",
    "            X = np.hstack((np.ones((X.shape[0],1)), X)) #add te column wit '1's\n",
    "            w = np.dot(np.dot( np.linalg.inv( np.dot(X.T, X) ), X.T), D)\n",
    "            self.w = w[1:]\n",
    "            self.b = w[0]\n",
    "            \n",
    "        pass\n",
    "    def Train(self, X, D, eta, epochs):\n",
    "        \"\"\"\n",
    "        Train for the maximum number of epochs or until the classification error is 0\n",
    "        X: matrix with examples, each examples as a row\n",
    "        D: vector of correct class labels for examples in rows of X\n",
    "        The update to the weights vector is done once per epoch, based on all examples\n",
    "        \"\"\"\n",
    "        n=0\n",
    "        while n<epochs and self.CalculateErrors(X,D)!=0:\n",
    "            self.Update(X,D,eta)\n",
    "            n+=1\n",
    "            \n",
    "        pass\n",
    "    def CalculateErrors(self, X, D):\n",
    "        \"\"\"Calculates the number of errors - missclassifications\"\"\"\n",
    "        \n",
    "        errors=np.sum(D!=self.Forward(X)) \n",
    "        return errors \n",
    "    \n",
    "        pass\n",
    "    \n",
    "\n",
    "##############################################################################\n",
    "#DO NOT CHANGE THE FOLLOWING CODE\n",
    "##############################################################################\n",
    "print('Loading train data...')\n",
    "train_data = np.loadtxt('train10D.csv')\n",
    "X = train_data[:,:-1]\n",
    "D = train_data[:,-1]\n",
    "num_of_inputs = X.shape[1]\n",
    "print('Train data:')\n",
    "print('Number of examples=',X.shape[0])\n",
    "print('Number of inputs=',num_of_inputs)\n",
    "\n",
    "perc = PerceptronBatch(num_of_inputs)\n",
    "perc.InitWeights()\n",
    "\n",
    "start_errors = perc.CalculateErrors(X,D)\n",
    "print('Initial number of errors=',start_errors)\n",
    "\n",
    "print('Training...')\n",
    "max_epochs = 100\n",
    "eta = 0.01\n",
    "perc.Train(X, D, eta, max_epochs)\n",
    "print('End of training')\n",
    "\n",
    "train_errors = perc.CalculateErrors(X,D)\n",
    "print('Errors for train data after training=',train_errors)\n",
    "\n",
    "print('Loading test data...')\n",
    "test_data = np.loadtxt('test10D.csv')\n",
    "print('Test data:')\n",
    "print('Number of examples=',test_data.shape[0])\n",
    "print('Number of inputs=',test_data.shape[1])\n",
    "\n",
    "print('Calculating answers for test data...')\n",
    "test_ans = perc.Forward(test_data)\n",
    "print('Saving classifications for test data...')\n",
    "np.savetxt('test_data_classifications_percbatch.csv', test_ans)\n",
    "\n",
    "print('Checking test error...')\n",
    "true_test_labels = np.loadtxt('test10D_correct_ans.csv')\n",
    "test_errors = (true_test_labels != test_ans).sum()\n",
    "print('Test errors=',test_errors,' -> ',test_errors/float(test_data.shape[0]),'%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - Online version of Widrow-Hoff learning\n",
    "\n",
    "Expected test output:\n",
    "\n",
    "---CLASSIFICATION PROBLEM---\n",
    "\n",
    "Loading train data...\n",
    "\n",
    "Train data:\n",
    "\n",
    "Number of examples= 500\n",
    "\n",
    "Number of inputs= 10\n",
    "\n",
    "Initial number of errors= 241\n",
    "\n",
    "Initial MSE= 1.5702372419231343\n",
    "\n",
    "Training...\n",
    "\n",
    "End of training\n",
    "\n",
    "Errors for train data after training= 6\n",
    "\n",
    "MSE for train data after training= 0.31192984053640277\n",
    "\n",
    "Loading test data...\n",
    "\n",
    "Test data:\n",
    "\n",
    "Number of examples= 439\n",
    "\n",
    "Number of inputs= 10\n",
    "\n",
    "Calculating answers for test data...\n",
    "\n",
    "Saving classifications for test data...\n",
    "\n",
    "Checking test error...\n",
    "\n",
    "Test errors= 6  ->  0.01366742596810934 %\n",
    "\n",
    "\n",
    "---REGRESSION PROBLEM---\n",
    "\n",
    "x= [-6.  -5.5 -5.  -4.5 -4.  -3.5 -3.  -2.5 -2.  -1.5 -1.  -0.5  0.   0.5\n",
    "  1.   1.5  2.   2.5  3.   3.5  4.   4.5  5.   5.5]\n",
    "\n",
    "Initial MSE= 7.177903123126157\n",
    "\n",
    "Training for regression...\n",
    "\n",
    "After training, training MSE= 0.04428786345738948\n",
    "\n",
    "After training, testing MSE= 0.005203074366538246\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CLASSIFICATION PROBLEM---\n",
      "Loading train data...\n",
      "Train data:\n",
      "Number of examples= 500\n",
      "Number of inputs= 10\n",
      "Initial number of errors= 216\n",
      "Initial MSE= 0.058787753826796275\n",
      "Training...\n",
      "End of training\n",
      "Errors for train data after training= 0\n",
      "MSE for train data after training= 0.0\n",
      "Loading test data...\n",
      "Test data:\n",
      "Number of examples= 439\n",
      "Number of inputs= 10\n",
      "Calculating answers for test data...\n",
      "Saving classifications for test data...\n",
      "Checking test error...\n",
      "Test errors= 3  ->  0.00683371298405467 %\n",
      "\n",
      "---REGRESSION PROBLEM---\n",
      "x= [-6.  -5.5 -5.  -4.5 -4.  -3.5 -3.  -2.5 -2.  -1.5 -1.  -0.5  0.   0.5\n",
      "  1.   1.5  2.   2.5  3.   3.5  4.   4.5  5.   5.5]\n",
      "Initial MSE= 0.2070565556839953\n",
      "Training for regression...\n",
      "After training, training MSE= 0.03799096493496788\n",
      "After training, testing MSE= 0.013709058884728998\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "class WidrowHoffOnline:\n",
    "    \"\"\"\n",
    "    This is a Widrow-Hoff model which can process one example at a time.\n",
    "    Can be used for both classification and regression problems\n",
    "    Assumption: class label is given as {-1, 1} in classification problems\n",
    "    \"\"\"\n",
    "    def __init__(self, num_of_inputs):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "        self.w =  -1 +2*np.random.rand(num_of_inputs)  #neurons' weights as columns\n",
    "        self.b = 0.5 \n",
    "        self.outs = None\n",
    "        pass\n",
    "    def InitWeights(self):\n",
    "        \"\"\"Initializes weights to random values\"\"\"\n",
    "        pass\n",
    "    def Forward(self, x): \n",
    "        \"\"\"Forward pass - calculate the output as a real value of the neuron for one example x\"\"\"\n",
    "        self.outs = np.dot(x, self.w) + self.b\n",
    "        return self.outs\n",
    "    \n",
    "        pass\n",
    "    def ForwardClassify(self, x): \n",
    "        \"\"\"\n",
    "        Forward pass - calculate the output as {-1, 1} by comparing the real output value of the neuron with threshold 0; \n",
    "        for one example x\n",
    "        \"\"\"\n",
    "        self.outs = np.sign(np.dot(x, self.w) + self.b)\n",
    "        return self.outs\n",
    "        pass\n",
    "  \n",
    "\n",
    "    def Update(self, x, d, eta):\n",
    "        \"\"\"Calculate the output for x (one example), and update the weights\"\"\"\n",
    "        if(len(x)==1):\n",
    "            ans=self.Forward(x)\n",
    "            self.w += eta*(d-ans)*x\n",
    "            self.b += eta*(d-ans)\n",
    "        else :\n",
    "            ans=self.ForwardClassify(x)\n",
    "            self.w += eta*(d-ans)*x\n",
    "            self.b += eta*(d-ans)\n",
    "        pass\n",
    "    \n",
    "    def Train(self, X, D, eta, epochs):\n",
    "        \"\"\"\n",
    "        Train for the maximum number of epochs\n",
    "        X: matrix with examples, each examples as a row\n",
    "        D: vector of real values required for examples in rows of X \n",
    "        \"\"\"\n",
    "        n=0\n",
    "        while n<epochs:\n",
    "            for index, x in enumerate(X) :\n",
    "                self.Update(x,D[index],eta)\n",
    "            n+=1\n",
    "\n",
    "        pass \n",
    "        \n",
    "    \n",
    "    def CalculateErrors(self, X, D):\n",
    "        \"\"\"\n",
    "        Calculates the number of errors - missclassifications;\n",
    "        D - assumed to be {-1, 1} here\n",
    "        \"\"\"\n",
    "        #if(len(X[0])==1):\n",
    "        #    errors=np.sum(D!=np.array(self.Forward(x) for x in X))  \n",
    "        #else:\n",
    "        #    errors=np.sum(D!=np.array(self.ForwardClassify(x) for x in X))  \n",
    "        \n",
    "        errors=0\n",
    "        for index, d in enumerate(D):\n",
    "            errors+=d!=self.ForwardClassify(X[index]) \n",
    "         \n",
    "        return errors\n",
    "        pass\n",
    "    def CalculateMSE(self, X, D):\n",
    "        \"\"\"\n",
    "        Calculates the mean square error \n",
    "        D - assumed to be a vector of any real values here\n",
    "        \"\"\"\n",
    "        #self.Forward(X)\n",
    "        #self.mse = np.linalg.norm(np.array(self.Forward(x) for x in X) - D, axis=1).sum()/D.shape[0]\n",
    "        self.mse=0\n",
    "        #if(X.ndim==1):\n",
    "        if(len(X[0])==1):\n",
    "            self.mse = np.linalg.norm(self.Forward(X) - D).sum()/D.shape[0]\n",
    "        #    for x in X :\n",
    "         #       self.mse+=np.sqrt(((self.Forward(x)  - D) * (self.Forward(x)  - D)).sum())\n",
    "         #   self.mse/=D.shape[0]\n",
    "#            self.mse=np.sqrt(((self.Forward(x)  - D) * (self.Forward(x)  - D)).sum())/D.shape[0]\n",
    "        else:\n",
    "            self.mse = np.linalg.norm(self.ForwardClassify(X) - D).sum()/D.shape[0]\n",
    "       #     for x in X :\n",
    "       #         self.mse+=np.sqrt(((self.ForwardClassify(x)  - D) * (self.ForwardClassify(x)  - D)).sum())\n",
    "      #      self.mse/=D.shape[0]\n",
    "       # self.mse= np.dot((self.ForwardClassify(x)  - D).T, (self.ForwardClassify(x)  - D))\n",
    "        \n",
    "        return self.mse\n",
    "       # mse=0\n",
    "       # for index, d in enumerate(D):\n",
    "        #    mse+=self.mse = np.linalg.norm(self.outs - D, axis=1).sum()/D.shape[0]\n",
    "         \n",
    "            \n",
    "        pass\n",
    "   \n",
    "    \n",
    "\n",
    "##############################################################################\n",
    "#DO NOT CHANGE THE FOLLOWING CODE\n",
    "############################################################################## \n",
    "print('---CLASSIFICATION PROBLEM---')\n",
    "print('Loading train data...')\n",
    "train_data = np.loadtxt('train10D.csv')\n",
    "X = train_data[:,:-1]\n",
    "D = train_data[:,-1]\n",
    "num_of_inputs = X.shape[1]\n",
    "print('Train data:')\n",
    "print('Number of examples=',X.shape[0])\n",
    "print('Number of inputs=',num_of_inputs)\n",
    "\n",
    "perc = WidrowHoffOnline(num_of_inputs)\n",
    "perc.InitWeights()\n",
    "\n",
    "start_errors = perc.CalculateErrors(X,D)\n",
    "start_mse = perc.CalculateMSE(X,D)\n",
    "print('Initial number of errors=',start_errors)\n",
    "print('Initial MSE=',start_mse)\n",
    "\n",
    "print('Training...')\n",
    "max_epochs = 200\n",
    "eta = 0.001\n",
    "perc.Train(X, D, eta, max_epochs)\n",
    "print('End of training')\n",
    "\n",
    "train_errors = perc.CalculateErrors(X,D)\n",
    "train_mse = perc.CalculateMSE(X,D)\n",
    "print('Errors for train data after training=',train_errors)\n",
    "print('MSE for train data after training=',train_mse)\n",
    "\n",
    "print('Loading test data...')\n",
    "test_data = np.loadtxt('test10D.csv')\n",
    "print('Test data:')\n",
    "print('Number of examples=',test_data.shape[0])\n",
    "print('Number of inputs=',test_data.shape[1])\n",
    "\n",
    "print('Calculating answers for test data...')\n",
    "test_ans = []\n",
    "for x in test_data:\n",
    "    test_ans.append ( perc.ForwardClassify(x) )\n",
    "test_ans = np.array(test_ans)\n",
    "print('Saving classifications for test data...')\n",
    "np.savetxt('test_data_classifications_whonline.csv', test_ans)\n",
    "\n",
    "print('Checking test error...')\n",
    "true_test_labels = np.loadtxt('test10D_correct_ans.csv')\n",
    "test_errors = (true_test_labels != test_ans).sum()\n",
    "print('Test errors=',test_errors,' -> ',test_errors/float(test_data.shape[0]),'%')\n",
    "\n",
    "\n",
    "print()\n",
    "print('---REGRESSION PROBLEM---')\n",
    "xmin = -6\n",
    "xmax = 6\n",
    "x = np.arange(xmin, xmax, 0.5)\n",
    "print ('x=',x)\n",
    "\n",
    "#real values of unknown process\n",
    "a = 0.6\n",
    "b = -0.4\n",
    "d = a*x + b\n",
    "\n",
    "#training data with noise (e.g., measurement errors)\n",
    "sigma = 0.2\n",
    "tr_d = d + np.random.randn(len(d)) * sigma\n",
    "\n",
    "x.shape = (x.shape[0], 1)\n",
    "\n",
    "perc_reg = WidrowHoffOnline(1)\n",
    "start_mse = perc_reg.CalculateMSE(x, tr_d)\n",
    "print('Initial MSE=', start_mse)\n",
    "\n",
    "print('Training for regression...')\n",
    "eta = 0.01\n",
    "max_epochs = 100\n",
    "perc_reg.Train(x, tr_d, eta, max_epochs)\n",
    "\n",
    "train_mse = perc_reg.CalculateMSE(x, tr_d)\n",
    "print('After training, training MSE=', train_mse)\n",
    "\n",
    "#test data \n",
    "x_test = np.arange(xmin, xmax, 0.3)\n",
    "d_test = a*x_test + b\n",
    "x_test.shape = (x_test.shape[0],1)\n",
    "\n",
    "test_mse = perc_reg.CalculateMSE(x_test, d_test)\n",
    "print('After training, testing MSE=', test_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 - Batch version of Widrow-Hoff learning\n",
    "\n",
    "Expected test output:\n",
    "\n",
    "---CLASSIFICATION PROBLEM---\n",
    "\n",
    "Loading train data...\n",
    "\n",
    "Train data:\n",
    "\n",
    "Number of examples= 500\n",
    "\n",
    "Number of inputs= 10\n",
    "\n",
    "Initial number of errors= 320\n",
    "\n",
    "Initial MSE= 3.519674085434046\n",
    "\n",
    "Training...\n",
    "\n",
    "End of training\n",
    "\n",
    "Errors for train data after training= 6\n",
    "\n",
    "MSE for train data after training= 0.3119085202726676\n",
    "\n",
    "Loading test data...\n",
    "\n",
    "Test data:\n",
    "\n",
    "Number of examples= 439\n",
    "\n",
    "Number of inputs= 10\n",
    "\n",
    "Calculating answers for test data...\n",
    "\n",
    "Saving classifications for test data...\n",
    "\n",
    "Checking test error...\n",
    "\n",
    "Test errors= 6  ->  0.01366742596810934 %\n",
    "\n",
    "\n",
    "---REGRESSION PROBLEM---\n",
    "\n",
    "Initial MSE= 13.232938807228429\n",
    "\n",
    "Training for regression...\n",
    "\n",
    "After training, training MSE= 0.033557507870294406\n",
    "\n",
    "After training, testing MSE= 0.0035392872344154926\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CLASSIFICATION PROBLEM---\n",
      "Loading train data...\n",
      "Train data:\n",
      "Number of examples= 500\n",
      "Number of inputs= 10\n",
      "Initial number of errors= 227\n",
      "Initial MSE= 0.06026607669327746\n",
      "Training...\n",
      "End of training\n",
      "Errors for train data after training= 6\n",
      "MSE for train data after training= 0.009797958971132711\n",
      "Loading test data...\n",
      "Test data:\n",
      "Number of examples= 439\n",
      "Number of inputs= 10\n",
      "Calculating answers for test data...\n",
      "Saving classifications for test data...\n",
      "Checking test error...\n",
      "Test errors= 6  ->  0.01366742596810934 %\n",
      "\n",
      "---REGRESSION PROBLEM---\n",
      "Initial MSE= 0.2013532746965078\n",
      "Training for regression...\n",
      "After training, training MSE= 0.0330392205995251\n",
      "After training, testing MSE= 0.010327228425978502\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "class WidrowHoffBatch:\n",
    "    \"\"\"\n",
    "    This is a WidrowHoff model which can process all examples at a time.\n",
    "    Can be used for both classification and regression problems\n",
    "    Assumption: class label is given as {-1, 1} in classification problems\n",
    "    \"\"\"\n",
    "    def __init__(self, num_of_inputs):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "        self.w =  -1 +2*np.random.rand(num_of_inputs)  #neurons' weights as columns\n",
    "        self.b = 0.5 \n",
    "        self.outs = None\n",
    "        pass\n",
    "    def InitWeights(self):\n",
    "        \"\"\"Initializes weights to random values\"\"\"\n",
    "        pass\n",
    "    def Forward(self, X): \n",
    "        \"\"\"\n",
    "        Forward pass - calculate the output as a vector of real values of the neuron for all examples in X\n",
    "        X: matrix with examples as rows\n",
    "        \"\"\"\n",
    "        self.outs = np.dot(X, self.w) + self.b\n",
    "        return self.outs\n",
    "        pass\n",
    "    def ForwardClassify(self, X): \n",
    "        \"\"\"\n",
    "        Forward pass - calculate the output as a vector of {-1, 1} by comparing the real output values of the neuron with threshold 0; \n",
    "        X: matrix with examples as rows\n",
    "        \"\"\"\n",
    "        self.outs = np.sign(np.dot(X, self.w) + self.b)\n",
    "        return self.outs\n",
    "        pass    \n",
    "    def Update(self, X, D, eta):\n",
    "        \"\"\"Calculate the output for all examples in X (as rows), and update the weights \"\"\"\n",
    "        X = np.hstack((np.ones((X.shape[0],1)), X)) #add te column wit '1's\n",
    "        w = np.dot(np.dot( np.linalg.inv( np.dot(X.T, X) ), X.T), D)\n",
    "        self.w = w[1:]\n",
    "        self.b = w[0]\n",
    "        pass\n",
    "        \n",
    "    def Train(self, X, D, eta, epochs):\n",
    "        \"\"\"\n",
    "        Train for the maximum number of epochs\n",
    "        X: matrix with examples, each examples as a row\n",
    "        D: vector of real values required for examples in rows of X \n",
    "        \"\"\"\n",
    "        n=0\n",
    "        while n<epochs:\n",
    "            self.Update(X,D,eta) \n",
    "            n+=1\n",
    "        pass\n",
    "    def CalculateErrors(self, X, D):\n",
    "        \"\"\"\n",
    "        Calculates the number of errors - missclassifications\n",
    "        D - assumed to be {-1, 1} here\n",
    "        \"\"\"\n",
    "        if(len(X[0])==1):\n",
    "            errors=np.sum(D!=self.Forward(X))  \n",
    "        else:\n",
    "            errors=np.sum(D!=self.ForwardClassify(X))   \n",
    "            \n",
    "        return errors\n",
    "        pass\n",
    "    def CalculateMSE(self, X, D):\n",
    "        \"\"\"\n",
    "        Calculates the mean square error \n",
    "        D - assumed to be a vector of any real values here\n",
    "        \"\"\"\n",
    "\n",
    "        self.mse=0\n",
    "        if(len(X[0])==1):\n",
    "            self.mse = np.linalg.norm(self.Forward(X) - D).sum()/D.shape[0]\n",
    "        else:\n",
    "            self.mse = np.linalg.norm(self.ForwardClassify(X) - D).sum()/D.shape[0]\n",
    "  \n",
    "        return self.mse\n",
    " \n",
    "             \n",
    "        pass    \n",
    "    \n",
    "\n",
    "    \n",
    "##############################################################################\n",
    "#DO NOT CHANGE THE FOLLOWING CODE\n",
    "##############################################################################     \n",
    "print('---CLASSIFICATION PROBLEM---')\n",
    "print('Loading train data...')\n",
    "train_data = np.loadtxt('train10D.csv')\n",
    "X = train_data[:,:-1]\n",
    "D = train_data[:,-1]\n",
    "num_of_inputs = X.shape[1]\n",
    "print('Train data:')\n",
    "print('Number of examples=',X.shape[0])\n",
    "print('Number of inputs=',num_of_inputs)\n",
    "\n",
    "perc = WidrowHoffBatch(num_of_inputs)\n",
    "perc.InitWeights()\n",
    "\n",
    "start_errors = perc.CalculateErrors(X,D)\n",
    "start_mse = perc.CalculateMSE(X,D)\n",
    "print('Initial number of errors=',start_errors)\n",
    "print('Initial MSE=',start_mse)\n",
    "\n",
    "print('Training...')\n",
    "max_epochs = 100\n",
    "eta = 0.001\n",
    "perc.Train(X, D, eta, max_epochs)\n",
    "print('End of training')\n",
    "\n",
    "train_errors = perc.CalculateErrors(X,D)\n",
    "train_mse = perc.CalculateMSE(X,D)\n",
    "print('Errors for train data after training=',train_errors)\n",
    "print('MSE for train data after training=',train_mse)\n",
    "\n",
    "print('Loading test data...')\n",
    "test_data = np.loadtxt('test10D.csv')\n",
    "print('Test data:')\n",
    "print('Number of examples=',test_data.shape[0])\n",
    "print('Number of inputs=',test_data.shape[1])\n",
    "\n",
    "print('Calculating answers for test data...')\n",
    "test_ans = perc.ForwardClassify(test_data)\n",
    "print('Saving classifications for test data...')\n",
    "np.savetxt('test_data_classifications_whbatch.csv', test_ans)\n",
    "\n",
    "print('Checking test error...')\n",
    "true_test_labels = np.loadtxt('test10D_correct_ans.csv')\n",
    "test_errors = (true_test_labels != test_ans).sum()\n",
    "print('Test errors=',test_errors,' -> ',test_errors/float(test_data.shape[0]),'%')\n",
    "\n",
    "print()\n",
    "print('---REGRESSION PROBLEM---')\n",
    "xmin = -6\n",
    "xmax = 6\n",
    "x = np.arange(xmin, xmax, 0.5)\n",
    "\n",
    "#real values of unknown process\n",
    "a = 0.6\n",
    "b = -0.4\n",
    "d = a*x + b\n",
    "\n",
    "#training data with noise (e.g., measurement errors)\n",
    "sigma = 0.2\n",
    "tr_d = d + np.random.randn(len(d)) * sigma\n",
    "\n",
    "x.shape = (x.shape[0], 1)\n",
    "\n",
    "perc_reg = WidrowHoffBatch(1)\n",
    "start_mse = perc_reg.CalculateMSE(x, tr_d)\n",
    "print('Initial MSE=', start_mse)\n",
    "\n",
    "print('Training for regression...')\n",
    "eta = 0.001\n",
    "max_epochs = 100\n",
    "perc_reg.Train(x, tr_d, eta, max_epochs)\n",
    "\n",
    "train_mse = perc_reg.CalculateMSE(x, tr_d)\n",
    "print('After training, training MSE=', train_mse)\n",
    "\n",
    "#test data \n",
    "x_test = np.arange(xmin, xmax, 0.3)\n",
    "d_test = a*x_test + b\n",
    "x_test.shape = (x_test.shape[0],1)\n",
    "\n",
    "test_mse = perc_reg.CalculateMSE(x_test, d_test)\n",
    "print('After training, testing MSE=', test_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
