{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erasmus Neural Networks\n",
    "http://michalbereta.pl/nn\n",
    "## RBF Networks (Radial basis function networks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "Exacute the examples.\n",
    "\n",
    "Then, do the tasks and send back the notebook.\n",
    "\n",
    "Change the name of this notebook according to the schema: {YourSurname}\\_{YourFirstName}\\_{OriginalFileName}.\n",
    "\n",
    "Be sure to fill all places with \"YOUR ANSWER HERE\".\n",
    "\n",
    "When ready, send the notebook, with all the necessary files zipped, to the teacher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF network - class design\n",
    "\n",
    "https://en.wikipedia.org/wiki/Radial_basis_function_network\n",
    "\n",
    "```\n",
    "An input vector x is used as input to all radial basis functions, each with different parameters. The output of the network is a linear combination of the outputs from radial basis functions.\n",
    "```\n",
    "\n",
    "In order to calculate the response of `ith` hidden neuron for a given input `x`, in the following code we use the radial function as:\n",
    "\n",
    "$$ houput[i](x) = e^{-\\frac{||x-hcenters[i]||^2}{hsigmas[i]^2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "class RBFNN:\n",
    "    def __init__(self, inputs_num, hidden_num, output_num):#hidden_num=number of radial neurons in the hidden layer\n",
    "        pass\n",
    "    def Print(self):#print basic info about the network\n",
    "        pass\n",
    "    def Forward(self, inputs):\n",
    "        pass\n",
    "    def GetOutputs(self):#returns real valued outputs\n",
    "        pass\n",
    "    def GetPredictions(self):#returns class labels as 0,1,2,...\n",
    "        pass\n",
    "    def GetClassificationError(self, labels):\n",
    "        pass\n",
    "    def GetMSE(self, d):\n",
    "        pass\n",
    "    def GetMaxRadialValue(self, X):#helper function for vizualization\n",
    "        pass\n",
    "    def InitCenters(self, inputs, sigma):\n",
    "        pass\n",
    "    def TrainMPInv(self, X, d, sigma): #matrix pseudo inverse for the output layer\n",
    "        pass\n",
    "    def TrainBatch(self, X, d, labels, sigma, eta, max_iters): #Widrow-Hoff model for the output layer\n",
    "        pass\n",
    "###############################################################################        \n",
    "\n",
    "net = RBFNN(3, 4, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### RBF network - constructor\n",
    "\n",
    "- Weights of the linear output layer are randomly generated (as usual)\n",
    "\n",
    "- Parameters of radial hidden units are practically uninitialized here, we leave it for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "###############################################################################\n",
    "class RBFNN:\n",
    "    def __init__(self, inputs_num, hidden_num, output_num):#hidden_num=number of radial neurons in the hidden layer\n",
    "        self.inputs_num = inputs_num\n",
    "        self.hidden_num = hidden_num\n",
    "        self.output_num = output_num\n",
    "        self.hcenters = np.zeros((hidden_num, inputs_num)) #centres of radial functions in the hidden layer\n",
    "        self.hsigmas = np.ones(hidden_num)#sigma values of radial functions in the hidden layer\n",
    "        self.outweights = np.random.rand(hidden_num, output_num) #each output neuron as a column\n",
    "        self.outbiases = np.random.rand(output_num)#biases of the output linear neurons\n",
    "        self.houtputs = None #outputs of radial neurons (hidden layer)\n",
    "        self.netoutputs = None #output of the network (linear neurons)\n",
    "        self.stats = None #statistics about the MSE during batch training\n",
    "    def Print(self):#print basic info about the network\n",
    "        print('hcenters:\\n',self.hcenters)\n",
    "        print('hsigmas:\\n',self.hsigmas)\n",
    "        print('outweights:\\n', self.outweights)\n",
    "        print('outbiases:\\n',self.outbiases)        \n",
    "        if self.houtputs is not None:\n",
    "            print('houtputs:\\n',self.houtputs)\n",
    "        if self.netoutputs is not None:\n",
    "            print('netoutputs:\\n',self.netoutputs)  \n",
    "    def Forward(self, inputs):\n",
    "        pass\n",
    "    def GetOutputs(self):#returns real valued outputs\n",
    "        pass\n",
    "    def GetPredictions(self):#returns class labels as 0,1,2,...\n",
    "        pass\n",
    "    def GetClassificationError(self, labels):\n",
    "        pass\n",
    "    def GetMSE(self, d):\n",
    "        pass\n",
    "    def GetMaxRadialValue(self, X):#helper function for vizualization\n",
    "        pass\n",
    "    def InitCenters(self, inputs, sigma):\n",
    "        pass\n",
    "    def TrainMPInv(self, X, d, sigma): #matrix pseudo inverse\n",
    "        pass\n",
    "    def TrainBatch(self, X, d, labels, sigma, eta, max_iters): #Widrow-Hoff model\n",
    "        pass\n",
    "############################################################################### \n",
    "        \n",
    "net = RBFNN(3, 4, 2)\n",
    "net.Print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF network - forward pass, calculating the network responses\n",
    "\n",
    "First, the responses of radial hidden neurons are calculated. Then, based on them, the network's outputs are calculated as the outputs of linear Widorw-Hoff neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "class RBFNN:\n",
    "    def __init__(self, inputs_num, hidden_num, output_num):#hidden_num=number of radial neurons in the hidden layer\n",
    "        self.inputs_num = inputs_num\n",
    "        self.hidden_num = hidden_num\n",
    "        self.output_num = output_num\n",
    "        self.hcenters = np.zeros((hidden_num, inputs_num)) #centres of radial functions in the hidden layer\n",
    "        self.hsigmas = np.ones(hidden_num)#sigma values of radial functions in the hidden layer\n",
    "        self.outweights = np.random.rand(hidden_num, output_num) #each output neuron as a column\n",
    "        self.outbiases = np.random.rand(output_num)#biases of the output linear neurons\n",
    "        self.houtputs = None #outputs of radial neurons (hidden layer)\n",
    "        self.netoutputs = None #output of the network (linear neurons)\n",
    "        self.stats = None #statistics about the MSE during batch training\n",
    "    def Print(self):#print basic info about the network\n",
    "        print('hcenters:\\n',self.hcenters)\n",
    "        print('hsigmas:\\n',self.hsigmas)\n",
    "        print('outweights:\\n', self.outweights)\n",
    "        print('outbiases:\\n',self.outbiases)        \n",
    "        if self.houtputs is not None:\n",
    "            print('houtputs:\\n',self.houtputs)\n",
    "        if self.netoutputs is not None:\n",
    "            print('netoutputs:\\n',self.netoutputs)  \n",
    "    def Forward(self, inputs):\n",
    "        ##outputs of radial neurons (hidden layer)\n",
    "        self.houtputs = np.empty((inputs.shape[0], self.hcenters.shape[0]), dtype = float)\n",
    "        for i in range(inputs.shape[0]): #for each training example\n",
    "            self.houtputs[i,:] = np.exp(-np.sum((self.hcenters - inputs[i,:])**2, axis=1)/self.hsigmas**2)\n",
    "        ##outputs of linear neurons (output layer)\n",
    "        self.netoutputs = np.dot(self.houtputs, self.outweights) + self.outbiases\n",
    "    def GetOutputs(self):#returns real valued outputs\n",
    "        pass\n",
    "    def GetPredictions(self):#returns class labels as 0,1,2,...\n",
    "        pass\n",
    "    def GetClassificationError(self, labels):\n",
    "        pass\n",
    "    def GetMSE(self, d):\n",
    "        pass\n",
    "    def GetMaxRadialValue(self, X):#helper function for vizualization\n",
    "        pass\n",
    "    def InitCenters(self, inputs, sigma):\n",
    "        pass\n",
    "    def TrainMPInv(self, X, d, sigma): #matrix pseudo inverse\n",
    "        pass\n",
    "    def TrainBatch(self, X, d, labels, sigma, eta, max_iters): #Widrow-Hoff model\n",
    "        pass\n",
    "###############################################################################\n",
    "        \n",
    "ins = np.random.rand(5,3)\n",
    "net = RBFNN(3, 4, 2)\n",
    "net.Print()\n",
    "net.Forward(ins)\n",
    "print('------------------------')\n",
    "net.Print() \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF network - getting network responses, calculating classification errors and MSE (mean squared error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "class RBFNN:\n",
    "    def __init__(self, inputs_num, hidden_num, output_num):#hidden_num=number of radial neurons in the hidden layer\n",
    "        self.inputs_num = inputs_num\n",
    "        self.hidden_num = hidden_num\n",
    "        self.output_num = output_num\n",
    "        self.hcenters = np.zeros((hidden_num, inputs_num)) #centres of radial functions in the hidden layer\n",
    "        self.hsigmas = np.ones(hidden_num)#sigma values of radial functions in the hidden layer\n",
    "        self.outweights = np.random.rand(hidden_num, output_num) #each output neuron as a column\n",
    "        self.outbiases = np.random.rand(output_num)#biases of the output linear neurons\n",
    "        self.houtputs = None #outputs of radial neurons (hidden layer)\n",
    "        self.netoutputs = None #output of the network (linear neurons)\n",
    "        self.stats = None #statistics about the MSE during batch training\n",
    "    def Print(self):#print basic info about the network\n",
    "        print('hcenters:\\n',self.hcenters)\n",
    "        print('hsigmas:\\n',self.hsigmas)\n",
    "        print('outweights:\\n', self.outweights)\n",
    "        print('outbiases:\\n',self.outbiases)        \n",
    "        if self.houtputs is not None:\n",
    "            print('houtputs:\\n',self.houtputs)\n",
    "        if self.netoutputs is not None:\n",
    "            print('netoutputs:\\n',self.netoutputs)  \n",
    "    def Forward(self, inputs):\n",
    "        ##outputs of radial neurons (hidden layer)\n",
    "        self.houtputs = np.empty((inputs.shape[0], self.hcenters.shape[0]), dtype = float)\n",
    "        for i in range(inputs.shape[0]): #for each training example\n",
    "            self.houtputs[i,:] = np.exp(-np.sum((self.hcenters - inputs[i,:])**2, axis=1)/self.hsigmas**2)\n",
    "        ##outputs of linear neurons (output layer)\n",
    "        self.netoutputs = np.dot(self.houtputs, self.outweights) + self.outbiases\n",
    "    def GetOutputs(self):#returns real valued outputs\n",
    "        return self.netoutputs\n",
    "    def GetPredictions(self):#returns class labels as 0,1,2,...\n",
    "        return np.argmax(self.netoutputs, axis=1)\n",
    "    def GetClassificationError(self, labels):\n",
    "        return np.sum(labels!=self.GetPredictions())  \n",
    "    def GetMSE(self, d):\n",
    "        self.mse = ((self.netoutputs - d)*(self.netoutputs - d)).sum(axis=1).sum() /d.shape[0]\n",
    "        return self.mse       \n",
    "    def GetMaxRadialValue(self, X):#helper function for vizualization; for each example (row in X) returns the maximum value of any of the radial functions\n",
    "        self.Forward(X)\n",
    "        return self.houtputs.max(axis=1)\n",
    "    def InitCenters(self, inputs, sigma):\n",
    "        pass\n",
    "    def TrainMPInv(self, X, d, sigma): #matrix pseudo inverse\n",
    "        pass\n",
    "    def TrainBatch(self, X, d, labels, sigma, eta, max_iters): #Widrow-Hoff model\n",
    "        pass\n",
    "###############################################################################\n",
    "        \n",
    "ins = np.random.rand(5,3)\n",
    "net = RBFNN(3, 4, 2)\n",
    "net.Print()\n",
    "net.Forward(ins)\n",
    "print('------------------------')\n",
    "net.Print() \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF network - initializing the centres of radial functions based on training data\n",
    "\n",
    "Radial neurons have to be properly placed in the input space. The idea here is as follows: \n",
    "\n",
    "- select randomly a given number of training examples without repetitions, the number equals the number of hidden neurons;\n",
    "\n",
    "- use the positions of selected training examples as the centers of radial neurons;\n",
    "\n",
    "- sigma values will be adjusted by trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "class RBFNN:\n",
    "    def __init__(self, inputs_num, hidden_num, output_num):#hidden_num=number of radial neurons in the hidden layer\n",
    "        self.inputs_num = inputs_num\n",
    "        self.hidden_num = hidden_num\n",
    "        self.output_num = output_num\n",
    "        self.hcenters = np.zeros((hidden_num, inputs_num)) #centres of radial functions in the hidden layer\n",
    "        self.hsigmas = np.ones(hidden_num)#sigma values of radial functions in the hidden layer\n",
    "        self.outweights = np.random.rand(hidden_num, output_num) #each output neuron as a column\n",
    "        self.outbiases = np.random.rand(output_num)#biases of the output linear neurons\n",
    "        self.houtputs = None #outputs of radial neurons (hidden layer)\n",
    "        self.netoutputs = None #output of the network (linear neurons)\n",
    "        self.stats = None #statistics about the MSE during batch training\n",
    "    def Print(self):#print basic info about the network\n",
    "        print('hcenters:\\n',self.hcenters)\n",
    "        print('hsigmas:\\n',self.hsigmas)\n",
    "        print('outweights:\\n', self.outweights)\n",
    "        print('outbiases:\\n',self.outbiases)        \n",
    "        if self.houtputs is not None:\n",
    "            print('houtputs:\\n',self.houtputs)\n",
    "        if self.netoutputs is not None:\n",
    "            print('netoutputs:\\n',self.netoutputs)  \n",
    "    def Forward(self, inputs):\n",
    "        ##outputs of radial neurons (hidden layer)\n",
    "        self.houtputs = np.empty((inputs.shape[0], self.hcenters.shape[0]), dtype = float)\n",
    "        for i in range(inputs.shape[0]): #for each training example\n",
    "            self.houtputs[i,:] = np.exp(-np.sum((self.hcenters - inputs[i,:])**2, axis=1)/self.hsigmas**2)\n",
    "        ##outputs of linear neurons (output layer)\n",
    "        self.netoutputs = np.dot(self.houtputs, self.outweights) + self.outbiases\n",
    "    def GetOutputs(self):#returns real valued outputs\n",
    "        return self.netoutputs\n",
    "    def GetPredictions(self):#returns class labels as 0,1,2,...\n",
    "        return np.argmax(self.netoutputs, axis=1)\n",
    "    def GetClassificationError(self, labels):\n",
    "        return np.sum(labels!=self.GetPredictions())  \n",
    "    def GetMSE(self, d):\n",
    "        self.mse = ((self.netoutputs - d)*(self.netoutputs - d)).sum(axis=1).sum() /d.shape[0]\n",
    "        return self.mse       \n",
    "    def GetMaxRadialValue(self, X):#helper function for vizualization; for each example (row in X) returns the maximum value of any of the radial functions\n",
    "        self.Forward(X)\n",
    "        return self.houtputs.max(axis=1)\n",
    "    def InitCenters(self, inputs, sigma):#randomly select a self.hidden_num number of training examples and copy their positions as centres of rbf neurons\n",
    "        self.hsigmas = np.ones(self.hidden_num)*sigma\n",
    "        indxs = set()\n",
    "        while len(indxs) < self.hcenters.shape[0]:\n",
    "            indxs.add(np.random.randint(0,inputs.shape[0]))\n",
    "        self.hcenters = inputs[np.asarray(list(indxs)), :].copy()\n",
    "    def TrainMPInv(self, X, d, sigma): #matrix pseudo inverse\n",
    "        pass\n",
    "    def TrainBatch(self, X, d, labels, sigma, eta, max_iters): #Widrow-Hoff model\n",
    "        pass\n",
    "###############################################################################\n",
    "###############################################################################        \n",
    "def encode_labels_as_binary(d, num_of_classes):\n",
    "    rows = d.shape[0]\n",
    "    labels = -1*np.ones((rows, num_of_classes), dtype='float32')\n",
    "    labels[np.arange(rows),d.T] = 1\n",
    "    return labels\n",
    "###############################################################################        \n",
    "X = np.loadtxt('data_3classes_nonlinear_2D.txt')\n",
    "#print('X=',X)\n",
    "\n",
    "d = X[:,-1].astype('int')\n",
    "X = X[:,:-1]\n",
    "\n",
    "plt.figure()\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.plot(X[d==0,0],X[d==0,1], 'ro')\n",
    "plt.plot(X[d==1,0],X[d==1,1], 'go')\n",
    "plt.plot(X[d==2,0],X[d==2,1], 'bo')\n",
    "#plt.show()\n",
    "\n",
    "num_of_cls = len(set(d))\n",
    "num_of_ins = X.shape[1]\n",
    "\n",
    "print('num_of_cls=',num_of_cls)\n",
    "print('num_of_ins=',num_of_ins)\n",
    "\n",
    "dtrain = encode_labels_as_binary(d, num_of_cls)\n",
    "#print('dtrain=',dtrain)\n",
    "\n",
    "#experiment with the values of hidden_num and sigma, so that the training data is well covered by radial responses\n",
    "hidden_num = 5 #experiment with this value\n",
    "sigma = 0.1 #experiment with this value\n",
    "\n",
    "net = RBFNN(num_of_ins, hidden_num, num_of_cls)\n",
    "net.Forward(X)\n",
    "net.Print()\n",
    "\n",
    "#no training\n",
    "net.InitCenters(X, sigma)\n",
    "##########################################################################\n",
    "plt.figure()\n",
    "xmin = 0\n",
    "xmax = 1\n",
    "ymin = 0\n",
    "ymax = 1\n",
    "plt.xlim(xmin,xmax)\n",
    "plt.ylim(ymin,ymax)\n",
    "\n",
    "delta = 0.01 #accuracy of the grid for vizualization only\n",
    "_x = np.arange(xmin, xmax, delta)\n",
    "_y = np.arange(ymin, ymax, delta)\n",
    "_X, _Y = np.meshgrid(_x, _y)\n",
    "xx = _X.reshape(-1,1)\n",
    "yy = _Y.reshape(-1,1)\n",
    "Z = net.GetMaxRadialValue( np.hstack((xx, yy)) ).reshape(_X.shape)\n",
    "\n",
    "plt.plot(X[d==0,0], X[d==0,1],'ro')\n",
    "plt.plot(X[d==1,0], X[d==1,1],'go')\n",
    "plt.plot(X[d==2,0], X[d==2,1],'bo')\n",
    "\n",
    "im = plt.imshow(Z, interpolation='bilinear', cmap=cm.hot,\n",
    "            origin='lower', extent=[xmin, xmax, ymin, ymax],\n",
    "            vmax=Z.max(), vmin=Z.min())\n",
    "plt.colorbar()\n",
    "plt.title('max radial values')\n",
    "##########################################################################\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF network - train the output layer's weights using matrix pseudo-inverse\n",
    "\n",
    "Experiment with different numbers of hidden radial neurons and sigma values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "class RBFNN:\n",
    "    def __init__(self, inputs_num, hidden_num, output_num):#hidden_num=number of radial neurons in the hidden layer\n",
    "        self.inputs_num = inputs_num\n",
    "        self.hidden_num = hidden_num\n",
    "        self.output_num = output_num\n",
    "        self.hcenters = np.zeros((hidden_num, inputs_num)) #centres of radial functions in the hidden layer\n",
    "        self.hsigmas = np.ones(hidden_num)#sigma values of radial functions in the hidden layer\n",
    "        self.outweights = np.random.rand(hidden_num, output_num) #each output neuron as a column\n",
    "        self.outbiases = np.random.rand(output_num)#biases of the output linear neurons\n",
    "        self.houtputs = None #outputs of radial neurons (hidden layer)\n",
    "        self.netoutputs = None #output of the network (linear neurons)\n",
    "        self.stats = None #statistics about the MSE during batch training\n",
    "    def Print(self):#print basic info about the network\n",
    "        print('hcenters:\\n',self.hcenters)\n",
    "        print('hsigmas:\\n',self.hsigmas)\n",
    "        print('outweights:\\n', self.outweights)\n",
    "        print('outbiases:\\n',self.outbiases)        \n",
    "        if self.houtputs is not None:\n",
    "            print('houtputs:\\n',self.houtputs)\n",
    "        if self.netoutputs is not None:\n",
    "            print('netoutputs:\\n',self.netoutputs)  \n",
    "    def Forward(self, inputs):\n",
    "        ##outputs of radial neurons (hidden layer)\n",
    "        self.houtputs = np.empty((inputs.shape[0], self.hcenters.shape[0]), dtype = float)\n",
    "        for i in range(inputs.shape[0]): #for each training example\n",
    "            self.houtputs[i,:] = np.exp(-np.sum((self.hcenters - inputs[i,:])**2, axis=1)/self.hsigmas**2)\n",
    "        ##outputs of linear neurons (output layer)\n",
    "        self.netoutputs = np.dot(self.houtputs, self.outweights) + self.outbiases\n",
    "    def GetOutputs(self):#returns real valued outputs\n",
    "        return self.netoutputs\n",
    "    def GetPredictions(self):#returns class labels as 0,1,2,...\n",
    "        return np.argmax(self.netoutputs, axis=1)\n",
    "    def GetClassificationError(self, labels):\n",
    "        return np.sum(labels!=self.GetPredictions())  \n",
    "    def GetMSE(self, d):\n",
    "        self.mse = ((self.netoutputs - d)*(self.netoutputs - d)).sum(axis=1).sum() /d.shape[0]\n",
    "        return self.mse       \n",
    "    def GetMaxRadialValue(self, X):#helper function for vizualization; for each example (row in X) returns the maximum value of any of the radial functions\n",
    "        self.Forward(X)\n",
    "        return self.houtputs.max(axis=1)\n",
    "    def InitCenters(self, inputs, sigma):#randomly select a self.hidden_num number of training examples and copy their positions as centres of rbf neurons\n",
    "        self.hsigmas = np.ones(self.hidden_num)*sigma\n",
    "        indxs = set()\n",
    "        while len(indxs) < self.hcenters.shape[0]:\n",
    "            indxs.add(np.random.randint(0,inputs.shape[0]))\n",
    "        self.hcenters = inputs[np.asarray(list(indxs)), :].copy()\n",
    "    def TrainMPInv(self, X, d, sigma): #matrix pseudo inverse\n",
    "        self.InitCenters(X, sigma)\n",
    "        self.Forward(X)\n",
    "        #now the matrix pseudoinverse for the weights of the output linear neurons\n",
    "        r = np.hstack((np.ones((self.houtputs.shape[0], 1)), self.houtputs))\n",
    "        w = np.dot(np.dot( np.linalg.inv( np.dot(r.T, r) ), r.T), d)\n",
    "        self.w = w[1:,:]\n",
    "        self.b = w[0,:]\n",
    "    def TrainBatch(self, X, d, labels, sigma, eta, max_iters): #Widrow-Hoff model\n",
    "        pass\n",
    "###############################################################################\n",
    "###############################################################################        \n",
    "def encode_labels_as_binary(d, num_of_classes):\n",
    "    rows = d.shape[0]\n",
    "    labels = -1*np.ones((rows, num_of_classes), dtype='float32')\n",
    "    labels[np.arange(rows),d.T] = 1\n",
    "    return labels\n",
    "###############################################################################        \n",
    "X = np.loadtxt('data_3classes_nonlinear_2D.txt')\n",
    "#print('X=',X)\n",
    "\n",
    "d = X[:,-1].astype('int')\n",
    "X = X[:,:-1]\n",
    "\n",
    "plt.figure()\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.plot(X[d==0,0],X[d==0,1], 'ro')\n",
    "plt.plot(X[d==1,0],X[d==1,1], 'go')\n",
    "plt.plot(X[d==2,0],X[d==2,1], 'bo')\n",
    "#plt.show()\n",
    "\n",
    "num_of_cls = len(set(d))\n",
    "num_of_ins = X.shape[1]\n",
    "\n",
    "print('num_of_cls=',num_of_cls)\n",
    "print('num_of_ins=',num_of_ins)\n",
    "\n",
    "dtrain = encode_labels_as_binary(d, num_of_cls)\n",
    "#print('dtrain=',dtrain)\n",
    "\n",
    "net = RBFNN(num_of_ins, 100, num_of_cls)\n",
    "net.Print()\n",
    "net.Forward(X)\n",
    "net.Print()\n",
    "print('MSE before training=',net.GetMSE(dtrain))\n",
    "print('Classification error before training=',net.GetClassificationError(d))\n",
    "\n",
    "\n",
    "sigma = 0.3\n",
    "net.TrainMPInv(X, dtrain, sigma)\n",
    "\n",
    "net.Forward(X)\n",
    "#net.Print()\n",
    "print('MSE after training=',net.GetMSE(dtrain))\n",
    "print('Classification error after training=',net.GetClassificationError(d))\n",
    "print('houts max=',net.houtputs.max(axis=1).min())\n",
    "print('out w max=',net.outweights.max())\n",
    "print('out w min=',net.outweights.min())\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "plt.figure()\n",
    "xmin = 0\n",
    "xmax = 1\n",
    "ymin = 0\n",
    "ymax = 1\n",
    "plt.xlim(xmin,xmax)\n",
    "plt.ylim(ymin,ymax)\n",
    "\n",
    "delta = 0.01 #accuracy of the grid for vizualization only\n",
    "_x = np.arange(xmin, xmax, delta)\n",
    "_y = np.arange(ymin, ymax, delta)\n",
    "_X, _Y = np.meshgrid(_x, _y)\n",
    "xx = _X.reshape(-1,1)\n",
    "yy = _Y.reshape(-1,1)\n",
    "Z = net.GetMaxRadialValue( np.hstack((xx, yy)) ).reshape(_X.shape)\n",
    "\n",
    "plt.plot(X[d==0,0], X[d==0,1],'ro')\n",
    "plt.plot(X[d==1,0], X[d==1,1],'go')\n",
    "plt.plot(X[d==2,0], X[d==2,1],'bo')\n",
    "\n",
    "im = plt.imshow(Z, interpolation='bilinear', cmap=cm.hot,\n",
    "            origin='lower', extent=[xmin, xmax, ymin, ymax],\n",
    "            vmax=Z.max(), vmin=Z.min())\n",
    "plt.colorbar()\n",
    "plt.title('max radial values')\n",
    "##########################################################################\n",
    "plt.figure()\n",
    "xmin = 0\n",
    "xmax = 1\n",
    "ymin = 0\n",
    "ymax = 1\n",
    "plt.xlim(xmin,xmax)\n",
    "plt.ylim(ymin,ymax)\n",
    "\n",
    "delta = 0.01 #accuracy of the grid for vizualization only\n",
    "_x = np.arange(xmin, xmax, delta)\n",
    "_y = np.arange(ymin, ymax, delta)\n",
    "_X, _Y = np.meshgrid(_x, _y)\n",
    "xx = _X.reshape(-1,1)\n",
    "yy = _Y.reshape(-1,1)\n",
    "net.Forward(np.hstack((xx, yy)))\n",
    "Z = net.GetPredictions().reshape(_X.shape)\n",
    "\n",
    "plt.plot(X[d==0,0], X[d==0,1],'ro')\n",
    "plt.plot(X[d==1,0], X[d==1,1],'go')\n",
    "plt.plot(X[d==2,0], X[d==2,1],'bo')\n",
    "\n",
    "im = plt.imshow(Z, interpolation='bilinear', cmap=cm.hot,\n",
    "            origin='lower', extent=[xmin, xmax, ymin, ymax],\n",
    "            vmax=Z.max(), vmin=Z.min())\n",
    "plt.colorbar()\n",
    "plt.title('class boundaries')\n",
    "##########################################################################\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF network - train the output layer's weights using iterative algorithm (Widrow-Hoff, delta rule)\n",
    "\n",
    "Note the division in:\n",
    "\n",
    "```\n",
    "self.outweights += eta*np.dot(self.houtputs.T, d - self.netoutputs)/X.shape[0]\n",
    "self.outbiases += eta*np.dot(np.ones((1,self.houtputs.shape[0])), d - self.netoutputs).flatten()/X.shape[0]\n",
    "```\n",
    "\n",
    "The devision is done to avoid too strong gradients, when the number of training examples is huge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "class RBFNN:\n",
    "    def __init__(self, inputs_num, hidden_num, output_num):#hidden_num=number of radial neurons in the hidden layer\n",
    "        self.inputs_num = inputs_num\n",
    "        self.hidden_num = hidden_num\n",
    "        self.output_num = output_num\n",
    "        self.hcenters = np.zeros((hidden_num, inputs_num)) #centres of radial functions in the hidden layer\n",
    "        self.hsigmas = np.ones(hidden_num)#sigma values of radial functions in the hidden layer\n",
    "        self.outweights = np.random.rand(hidden_num, output_num) #each output neuron as a column\n",
    "        self.outbiases = np.random.rand(output_num)#biases of the output linear neurons\n",
    "        self.houtputs = None #outputs of radial neurons (hidden layer)\n",
    "        self.netoutputs = None #output of the network (linear neurons)\n",
    "        self.stats = None #statistics about the MSE during batch training\n",
    "    def Print(self):#print basic info about the network\n",
    "        print('hcenters:\\n',self.hcenters)\n",
    "        print('hsigmas:\\n',self.hsigmas)\n",
    "        print('outweights:\\n', self.outweights)\n",
    "        print('outbiases:\\n',self.outbiases)        \n",
    "        if self.houtputs is not None:\n",
    "            print('houtputs:\\n',self.houtputs)\n",
    "        if self.netoutputs is not None:\n",
    "            print('netoutputs:\\n',self.netoutputs)  \n",
    "    def Forward(self, inputs):\n",
    "        ##outputs of radial neurons (hidden layer)\n",
    "        self.houtputs = np.empty((inputs.shape[0], self.hcenters.shape[0]), dtype = float)\n",
    "        for i in range(inputs.shape[0]): #for each training example\n",
    "            self.houtputs[i,:] = np.exp(-np.sum((self.hcenters - inputs[i,:])**2, axis=1)/self.hsigmas**2)\n",
    "        ##outputs of linear neurons (output layer)\n",
    "        self.netoutputs = np.dot(self.houtputs, self.outweights) + self.outbiases\n",
    "    def GetOutputs(self):#returns real valued outputs\n",
    "        return self.netoutputs\n",
    "    def GetPredictions(self):#returns class labels as 0,1,2,...\n",
    "        return np.argmax(self.netoutputs, axis=1)\n",
    "    def GetClassificationError(self, labels):\n",
    "        return np.sum(labels!=self.GetPredictions())  \n",
    "    def GetMSE(self, d):\n",
    "        self.mse = ((self.netoutputs - d)*(self.netoutputs - d)).sum(axis=1).sum() /d.shape[0]\n",
    "        return self.mse       \n",
    "    def GetMaxRadialValue(self, X):#helper function for vizualization; for each example (row in X) returns the maximum value of any of the radial functions\n",
    "        self.Forward(X)\n",
    "        return self.houtputs.max(axis=1)\n",
    "    def InitCenters(self, inputs, sigma):#randomly select a self.hidden_num number of training examples and copy their positions as centres of rbf neurons\n",
    "        self.hsigmas = np.ones(self.hidden_num)*sigma\n",
    "        indxs = set()\n",
    "        while len(indxs) < self.hcenters.shape[0]:\n",
    "            indxs.add(np.random.randint(0,inputs.shape[0]))\n",
    "        self.hcenters = inputs[np.asarray(list(indxs)), :].copy()\n",
    "    def TrainMPInv(self, X, d, sigma): #matrix pseudo inverse\n",
    "        self.InitCenters(X, sigma)\n",
    "        self.Forward(X)\n",
    "        #now the matrix pseudoinverse for the weights of the output linear neurons\n",
    "        r = np.hstack((np.ones((self.houtputs.shape[0], 1)), self.houtputs))\n",
    "        w = np.dot(np.dot( np.linalg.inv( np.dot(r.T, r) ), r.T), d)\n",
    "        self.w = w[1:,:]\n",
    "        self.b = w[0,:]\n",
    "    def TrainBatch(self, X, d, labels, sigma, eta, max_iters): #Widrow-Hoff model, delta rule\n",
    "        self.InitCenters(X, sigma)\n",
    "        self.Forward(X)\n",
    "        self.stats = []\n",
    "        for i in range(max_iters):\n",
    "            self.outweights += eta*np.dot(self.houtputs.T, d - self.netoutputs)/X.shape[0]\n",
    "            self.outbiases += eta*np.dot(np.ones((1,self.houtputs.shape[0])), d - self.netoutputs).flatten()/X.shape[0]\n",
    "            self.Forward(X)\n",
    "            mse = self.GetMSE(d)\n",
    "            self.stats.append(mse)\n",
    "            print('mse=',mse)\n",
    "            classification_error = self.GetClassificationError(labels)\n",
    "            print('classification_error=',classification_error)\n",
    "            print()            \n",
    "###############################################################################\n",
    "###############################################################################        \n",
    "def encode_labels_as_binary(d, num_of_classes):\n",
    "    rows = d.shape[0]\n",
    "    labels = -1*np.ones((rows, num_of_classes), dtype='float32')\n",
    "    labels[np.arange(rows),d.T] = 1\n",
    "    return labels\n",
    "###############################################################################        \n",
    "X = np.loadtxt('data_3classes_nonlinear_2D.txt')\n",
    "#print('X=',X)\n",
    "\n",
    "d = X[:,-1].astype('int')\n",
    "X = X[:,:-1]\n",
    "\n",
    "plt.figure()\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.plot(X[d==0,0],X[d==0,1], 'ro')\n",
    "plt.plot(X[d==1,0],X[d==1,1], 'go')\n",
    "plt.plot(X[d==2,0],X[d==2,1], 'bo')\n",
    "#plt.show()\n",
    "\n",
    "num_of_cls = len(set(d))\n",
    "num_of_ins = X.shape[1]\n",
    "\n",
    "print('num_of_cls=',num_of_cls)\n",
    "print('num_of_ins=',num_of_ins)\n",
    "\n",
    "dtrain = encode_labels_as_binary(d, num_of_cls)\n",
    "#print('dtrain=',dtrain)\n",
    "\n",
    "#experiment with the values of hidden_num and sigma, so that the training data is well covered by radial responses\n",
    "hidden_num = 100 #experiment with this value\n",
    "sigma = 0.1 #experiment with this value\n",
    "\n",
    "net = RBFNN(num_of_ins, hidden_num, num_of_cls)\n",
    "net.Print()\n",
    "net.Forward(X)\n",
    "net.Print()\n",
    "print('MSE before training=',net.GetMSE(dtrain))\n",
    "print('Classification error before training=',net.GetClassificationError(d))\n",
    "\n",
    "net.TrainBatch(X, dtrain, d, sigma, 0.05, 200)\n",
    "\n",
    "net.Forward(X)\n",
    "net.Print()\n",
    "print('MSE after training=',net.GetMSE(dtrain))\n",
    "print('Classification error after training=',net.GetClassificationError(d))\n",
    "print('houts max=',net.houtputs.max(axis=1).min())\n",
    "print('out w max=',net.outweights.max())\n",
    "print('out w min=',net.outweights.min())\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "plt.figure()\n",
    "xmin = 0\n",
    "xmax = 1\n",
    "ymin = 0\n",
    "ymax = 1\n",
    "plt.xlim(xmin,xmax)\n",
    "plt.ylim(ymin,ymax)\n",
    "\n",
    "delta = 0.01 #accuracy of the grid for vizualization only\n",
    "_x = np.arange(xmin, xmax, delta)\n",
    "_y = np.arange(ymin, ymax, delta)\n",
    "_X, _Y = np.meshgrid(_x, _y)\n",
    "xx = _X.reshape(-1,1)\n",
    "yy = _Y.reshape(-1,1)\n",
    "Z = net.GetMaxRadialValue( np.hstack((xx, yy)) ).reshape(_X.shape)\n",
    "\n",
    "plt.plot(X[d==0,0], X[d==0,1],'ro')\n",
    "plt.plot(X[d==1,0], X[d==1,1],'go')\n",
    "plt.plot(X[d==2,0], X[d==2,1],'bo')\n",
    "\n",
    "im = plt.imshow(Z, interpolation='bilinear', cmap=cm.hot,\n",
    "            origin='lower', extent=[xmin, xmax, ymin, ymax],\n",
    "            vmax=Z.max(), vmin=Z.min())\n",
    "plt.colorbar()\n",
    "plt.title('max radial values')\n",
    "##########################################################################\n",
    "if net.stats != None:\n",
    "    plt.figure()\n",
    "    plt.title('MSE in Widrow-Hoff RBF net')\n",
    "    plt.plot(net.stats)\n",
    "##########################################################################\n",
    "plt.figure()\n",
    "xmin = 0\n",
    "xmax = 1\n",
    "ymin = 0\n",
    "ymax = 1\n",
    "plt.xlim(xmin,xmax)\n",
    "plt.ylim(ymin,ymax)\n",
    "\n",
    "delta = 0.01 #accuracy of the grid for vizualization only\n",
    "_x = np.arange(xmin, xmax, delta)\n",
    "_y = np.arange(ymin, ymax, delta)\n",
    "_X, _Y = np.meshgrid(_x, _y)\n",
    "xx = _X.reshape(-1,1)\n",
    "yy = _Y.reshape(-1,1)\n",
    "net.Forward(np.hstack((xx, yy)))\n",
    "Z = net.GetPredictions().reshape(_X.shape)\n",
    "\n",
    "plt.plot(X[d==0,0], X[d==0,1],'ro')\n",
    "plt.plot(X[d==1,0], X[d==1,1],'go')\n",
    "plt.plot(X[d==2,0], X[d==2,1],'bo')\n",
    "\n",
    "im = plt.imshow(Z, interpolation='bilinear', cmap=cm.hot,\n",
    "            origin='lower', extent=[xmin, xmax, ymin, ymax],\n",
    "            vmax=Z.max(), vmin=Z.min())\n",
    "plt.colorbar()\n",
    "plt.title('class boundaries')\n",
    "##########################################################################\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "- Use the previous example code.\n",
    "\n",
    "- Experiment with different settings: number of hidden neurons, number of epoch, eta and sigma values. \n",
    "\n",
    "- What is the minimum number of hidden units you can find, so that the learning is still possible? What are the settings?\n",
    "\n",
    "- Write your answers and comments below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWERS GO IN THIS CELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "My results are as follows ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "- Use the RBF network for iris dataset.\n",
    "\n",
    "- Find the best settings (number of hidden neurons, sigma values, eta and epochs)\n",
    "\n",
    "- What are the best results (classification rates)?\n",
    "\n",
    "- Does the matrix psudo-inverse work for this data?\n",
    "\n",
    "- Remeber about normalizing the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWERS AND COMMENTS HERE\n",
    "\n",
    "My results are as follows ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "- Use the RBF network for pima-diabetes dataset. There are two classes, the last column indicates the class.\n",
    "\n",
    "- Find the best settings (number of hidden neurons, sigma values, eta and epochs)\n",
    "\n",
    "- What are the best results (classification rates)?\n",
    "\n",
    "- Does the matrix psudo-inverse work for this data?\n",
    "\n",
    "- Use also MCPerceptron class (or your implementation of Widrow-Hoff model) as a linear model. Compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWERS AND COMMENTS HERE\n",
    "\n",
    "My results are as follows ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
